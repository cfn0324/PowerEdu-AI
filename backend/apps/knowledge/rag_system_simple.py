"""
ç®€åŒ–çš„RAGç³»ç»Ÿå®ç°ï¼Œé¿å…sklearnä¾èµ–é—®é¢˜
"""
import os
import json
import logging
import hashlib
import asyncio
from typing import List, Dict, Optional, Tuple, Any
from datetime import datetime
import uuid

# æ–‡æ¡£å¤„ç†
try:
    import PyPDF2
    HAS_PDF = True
except ImportError:
    HAS_PDF = False

try:
    import docx
    HAS_DOCX = True
except ImportError:
    HAS_DOCX = False

try:
    from markdown import markdown
    from bs4 import BeautifulSoup
    HAS_MARKDOWN = True
except ImportError:
    HAS_MARKDOWN = False

# å‘é‡åŒ–å’Œæ£€ç´¢
import numpy as np

# æ–‡æœ¬å¤„ç†
try:
    import jieba
    HAS_JIEBA = True
except ImportError:
    HAS_JIEBA = False

import re

logger = logging.getLogger(__name__)


class DocumentProcessor:
    """æ–‡æ¡£å¤„ç†å™¨"""
    
    @staticmethod
    def process_file(file_path: str) -> Tuple[str, Dict]:
        """å¤„ç†å•ä¸ªæ–‡ä»¶"""
        file_ext = os.path.splitext(file_path)[1].lower()
        
        if file_ext == '.txt':
            return DocumentProcessor._process_txt(file_path)
        elif file_ext == '.md':
            return DocumentProcessor._process_markdown(file_path)
        elif file_ext == '.pdf' and HAS_PDF:
            return DocumentProcessor._process_pdf(file_path)
        elif file_ext in ['.docx', '.doc'] and HAS_DOCX:
            return DocumentProcessor._process_docx(file_path)
        elif file_ext == '.html':
            return DocumentProcessor._process_html(file_path)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}")
    
    @staticmethod
    def _process_txt(file_path: str) -> Tuple[str, Dict]:
        """å¤„ç†TXTæ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
        except UnicodeDecodeError:
            with open(file_path, 'r', encoding='gbk') as f:
                content = f.read()
        
        metadata = {
            'source': file_path,
            'type': 'txt',
            'size': len(content)
        }
        
        return content, metadata
    
    @staticmethod
    def _process_markdown(file_path: str) -> Tuple[str, Dict]:
        """å¤„ç†Markdownæ–‡ä»¶"""
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æå–çº¯æ–‡æœ¬ï¼ˆç§»é™¤markdownè¯­æ³•ï¼‰
        if HAS_MARKDOWN:
            html = markdown(content)
            soup = BeautifulSoup(html, 'html.parser')
            text = soup.get_text()
        else:
            # ç®€å•çš„markdownè¯­æ³•ç§»é™¤
            text = re.sub(r'[#*`_\[\]()]', '', content)
        
        metadata = {
            'source': file_path,
            'type': 'markdown',
            'size': len(text)
        }
        
        return text, metadata
    
    @staticmethod
    def _process_pdf(file_path: str) -> Tuple[str, Dict]:
        """å¤„ç†PDFæ–‡ä»¶"""
        text = ""
        with open(file_path, 'rb') as f:
            reader = PyPDF2.PdfReader(f)
            for page in reader.pages:
                text += page.extract_text() + "\n"
        
        metadata = {
            'source': file_path,
            'type': 'pdf',
            'pages': len(reader.pages),
            'size': len(text)
        }
        
        return text, metadata
    
    @staticmethod
    def _process_docx(file_path: str) -> Tuple[str, Dict]:
        """å¤„ç†DOCXæ–‡ä»¶"""
        doc = docx.Document(file_path)
        text = ""
        for paragraph in doc.paragraphs:
            text += paragraph.text + "\n"
        
        metadata = {
            'source': file_path,
            'type': 'docx',
            'paragraphs': len(doc.paragraphs),
            'size': len(text)
        }
        
        return text, metadata
    
    @staticmethod
    def _process_html(file_path: str) -> Tuple[str, Dict]:
        """å¤„ç†HTMLæ–‡ä»¶"""
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        if HAS_MARKDOWN:
            soup = BeautifulSoup(content, 'html.parser')
            text = soup.get_text()
        else:
            # ç®€å•çš„HTMLæ ‡ç­¾ç§»é™¤
            text = re.sub(r'<[^>]+>', '', content)
        
        metadata = {
            'source': file_path,
            'type': 'html',
            'size': len(text)
        }
        
        return text, metadata


class TextSplitter:
    """æ–‡æœ¬åˆ†å—å™¨"""
    
    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
    
    def split_text(self, text: str, metadata: Dict = None) -> List[Dict]:
        """åˆ†å‰²æ–‡æœ¬ä¸ºå—"""
        # ç®€å•çš„æ–‡æœ¬åˆ†å‰²
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + self.chunk_size
            chunk = text[start:end]
            
            # å°è¯•åœ¨å¥å·å¤„åˆ†å‰²
            if end < len(text):
                last_period = chunk.rfind('ã€‚')
                if last_period > self.chunk_size // 2:
                    chunk = chunk[:last_period + 1]
                    end = start + last_period + 1
            
            chunk_metadata = {
                'chunk_index': len(chunks),
                'chunk_size': len(chunk),
                'chunk_word_count': len(chunk.split()),
                **(metadata or {})
            }
            
            chunks.append({
                'content': chunk.strip(),
                'metadata': chunk_metadata
            })
            
            start = end - self.chunk_overlap
        
        return chunks


class SimpleEmbedding:
    """ç®€å•çš„åµŒå…¥æ¨¡å‹å®ç°"""
    
    def __init__(self, vector_size=300):
        self.vector_size = vector_size
        self.is_fitted = True  # ç®€åŒ–ç‰ˆæœ¬ï¼Œä¸éœ€è¦è®­ç»ƒ
        # é¢„å®šä¹‰è¯æ±‡è¡¨å’Œå“ˆå¸Œå‡½æ•°
        self.vocab = {}
        self.vocab_size = 1000  # é™åˆ¶è¯æ±‡è¡¨å¤§å°
    
    def _get_vocab(self, texts):
        """å»ºç«‹è¯æ±‡è¡¨"""
        vocab = set()
        for text in texts:
            # ç®€å•çš„ä¸­æ–‡åˆ†è¯ï¼ˆæŒ‰å­—ç¬¦ï¼‰
            chars = list(text)
            vocab.update(chars)
        
        # é™åˆ¶è¯æ±‡è¡¨å¤§å°ï¼Œé€‰æ‹©æœ€å¸¸è§çš„å­—ç¬¦
        if len(vocab) > self.vocab_size:
            from collections import Counter
            all_chars = []
            for text in texts:
                all_chars.extend(list(text))
            char_counts = Counter(all_chars)
            vocab = [char for char, count in char_counts.most_common(self.vocab_size)]
        else:
            vocab = list(vocab)
        
        return vocab
    
    def encode(self, texts: List[str]) -> np.ndarray:
        """ç¼–ç æ–‡æœ¬ä¸ºå‘é‡"""
        if not texts:
            return np.array([])
        
        # å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡ç¼–ç ï¼Œå»ºç«‹è¯æ±‡è¡¨
        if not self.vocab:
            vocab_list = self._get_vocab(texts)
            self.vocab = {char: i for i, char in enumerate(vocab_list)}
        
        vectors = []
        for text in texts:
            # åˆ›å»ºå›ºå®šå¤§å°çš„å‘é‡
            vector = np.zeros(self.vector_size)
            
            # å­—ç¬¦çº§åˆ«çš„ç®€å•ç¼–ç 
            chars = list(text)
            for i, char in enumerate(chars):
                if char in self.vocab:
                    idx = self.vocab[char]
                    # ä½¿ç”¨å“ˆå¸Œå‡½æ•°æ˜ å°„åˆ°å›ºå®šå¤§å°çš„å‘é‡
                    pos = idx % self.vector_size
                    vector[pos] += 1
            
            # å½’ä¸€åŒ–
            norm = np.linalg.norm(vector)
            if norm > 0:
                vector = vector / norm
            
            vectors.append(vector)
        
        return np.array(vectors, dtype=float)


class VectorStore:
    """å‘é‡å­˜å‚¨å™¨"""
    
    def __init__(self, embedding_model=None):
        self.embedding_model = embedding_model or SimpleEmbedding()
        self.chunks = []
        self.vectors = None
        self.metadata = []
    
    def add_documents(self, chunks: List[Dict]):
        """æ·»åŠ æ–‡æ¡£å—å¹¶æŒä¹…åŒ–åˆ°æ•°æ®åº“"""
        from django.db import transaction
        from apps.knowledge.models import DocumentChunk, Document
        
        with transaction.atomic():
            for chunk in chunks:
                self.chunks.append(chunk['content'])
                self.metadata.append(chunk['metadata'])
                
                # æŒä¹…åŒ–åˆ°æ•°æ®åº“
                if 'document_id' in chunk['metadata']:
                    try:
                        document = Document.objects.get(id=chunk['metadata']['document_id'])
                        DocumentChunk.objects.create(
                            document=document,
                            chunk_index=chunk['metadata'].get('chunk_index', 0),
                            content=chunk['content'],
                            embedding=None,  # å‘é‡å°†åœ¨_update_vectorsä¸­æ›´æ–°
                            metadata=chunk['metadata']
                        )
                    except Document.DoesNotExist:
                        logger.warning(f"Document with ID {chunk['metadata']['document_id']} not found")
        
        # é‡æ–°è®¡ç®—å‘é‡
        self._update_vectors()
    
    def _update_vectors(self):
        """æ›´æ–°å‘é‡å¹¶æŒä¹…åŒ–åˆ°æ•°æ®åº“"""
        if self.chunks:
            self.vectors = self.embedding_model.encode(self.chunks)
            
            # æ›´æ–°æ•°æ®åº“ä¸­çš„å‘é‡ - å¤„ç†å¼‚æ­¥ç¯å¢ƒ
            from apps.knowledge.models import DocumentChunk
            import asyncio
            import threading
            import queue
            
            # æ£€æŸ¥æ˜¯å¦åœ¨å¼‚æ­¥ç¯å¢ƒä¸­
            try:
                loop = asyncio.get_event_loop()
                if loop.is_running():
                    # åœ¨å¼‚æ­¥ç¯å¢ƒä¸­ï¼Œä½¿ç”¨çº¿ç¨‹æ¥æ‰§è¡ŒåŒæ­¥æ•°æ®åº“æ“ä½œ
                    def sync_update_embeddings():
                        for i, vector in enumerate(self.vectors):
                            if i < len(self.metadata) and 'document_id' in self.metadata[i]:
                                try:
                                    chunk = DocumentChunk.objects.filter(
                                        document_id=self.metadata[i]['document_id'],
                                        chunk_index=self.metadata[i].get('chunk_index', 0)
                                    ).first()
                                    if chunk:
                                        chunk.embedding = vector.tolist()
                                        chunk.save()
                                except Exception as e:
                                    logger.warning(f"Failed to update embedding for chunk {i}: {e}")
                    
                    # åœ¨ç‹¬ç«‹çº¿ç¨‹ä¸­æ‰§è¡Œæ•°æ®åº“æ›´æ–°
                    thread = threading.Thread(target=sync_update_embeddings)
                    thread.start()
                    thread.join()
                    return
                    
            except RuntimeError:
                # æ²¡æœ‰äº‹ä»¶å¾ªç¯ï¼Œå¯ä»¥è¿›è¡ŒåŒæ­¥æ“ä½œ
                pass
            
            # åŒæ­¥ç¯å¢ƒä¸­çš„æ­£å¸¸æ›´æ–°
            for i, vector in enumerate(self.vectors):
                if i < len(self.metadata) and 'document_id' in self.metadata[i]:
                    try:
                        chunk = DocumentChunk.objects.filter(
                            document_id=self.metadata[i]['document_id'],
                            chunk_index=self.metadata[i].get('chunk_index', 0)
                        ).first()
                        if chunk:
                            chunk.embedding = vector.tolist()
                            chunk.save()
                    except Exception as e:
                        logger.warning(f"Failed to update embedding for chunk {i}: {e}")
    
    def similarity_search(self, query: str, top_k: int = 5, threshold: float = 0.1) -> List[Dict]:
        """ç›¸ä¼¼åº¦æœç´¢"""
        if not self.chunks or self.vectors is None:
            return []
        
        # ç¼–ç æŸ¥è¯¢
        query_vector = self.embedding_model.encode([query])
        
        # ç®€å•çš„ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—
        def cosine_sim(a, b):
            dot_product = np.dot(a, b)
            norm_a = np.linalg.norm(a)
            norm_b = np.linalg.norm(b)
            if norm_a == 0 or norm_b == 0:
                return 0
            return dot_product / (norm_a * norm_b)
        
        similarities = []
        for vector in self.vectors:
            sim = cosine_sim(query_vector[0], vector)
            similarities.append(sim)
        similarities = np.array(similarities)
        
        # è·å–top_kç»“æœ
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        results = []
        for idx in top_indices:
            score = similarities[idx]
            if score >= threshold:
                results.append({
                    'content': self.chunks[idx],
                    'score': float(score),
                    'metadata': self.metadata[idx],
                    'index': int(idx)
                })
        
        return results


class LLMInterface:
    """å¤§è¯­è¨€æ¨¡å‹æ¥å£"""
    
    def __init__(self, model_config):
        """åˆå§‹åŒ–LLMæ¥å£
        
        Args:
            model_config: å¯ä»¥æ˜¯å­—å…¸æˆ–ModelConfigå¯¹è±¡
        """
        if hasattr(model_config, 'model_type'):
            # å¦‚æœæ˜¯ModelConfigå¯¹è±¡ï¼Œè½¬æ¢ä¸ºå­—å…¸
            self.model_config = {
                'model_type': model_config.model_type,
                'model_name': model_config.model_name,
                'api_key': model_config.api_key,
                'api_base_url': model_config.api_base_url,
                'max_tokens': model_config.max_tokens,
                'temperature': model_config.temperature,
            }
        else:
            # å¦‚æœæ˜¯å­—å…¸ï¼Œç›´æ¥ä½¿ç”¨
            self.model_config = model_config
        
        self.model_type = self.model_config.get('model_type', 'mock')
        self.model_name = self.model_config.get('model_name', 'mock')
    
    async def generate(self, prompt: str, context: str = "") -> str:
        """ç”Ÿæˆå›ç­”"""
        if self.model_type == 'mock' or self.model_name == 'mock':
            return f"åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š{context[:100]}...\n\nå¯¹äºé—®é¢˜ã€Œ{prompt}ã€ï¼Œè¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿå›ç­”ã€‚è¯·é…ç½®çœŸå®çš„å¤§è¯­è¨€æ¨¡å‹ä»¥è·å¾—å‡†ç¡®å›ç­”ã€‚"
        
        # çœŸå®APIè°ƒç”¨
        if self.model_type == 'api':
            try:
                return await self._call_real_api(prompt, context)
            except Exception as e:
                logger.error(f"APIè°ƒç”¨å¤±è´¥: {e}")
                return f"æŠ±æ­‰ï¼Œè°ƒç”¨AIæ¨¡å‹æ—¶å‡ºç°é”™è¯¯ï¼š{str(e)}"
        
        return "è¯·é…ç½®å¤§è¯­è¨€æ¨¡å‹"
    
    async def _call_real_api(self, prompt: str, context: str = "") -> str:
        """è°ƒç”¨çœŸå®çš„API"""
        import aiohttp
        import json
        
        api_key = self.model_config.get('api_key')
        api_base_url = self.model_config.get('api_base_url')
        model_name = self.model_config.get('model_name')
        
        if not api_key:
            return "é”™è¯¯ï¼šæœªé…ç½®APIå¯†é’¥"
        
        # ç›´æ¥ä½¿ç”¨ä¼ å…¥çš„promptï¼Œä¸å†é‡æ–°æ„å»º
        # å› ä¸ºåœ¨ask_questionä¸­å·²ç»æ„å»ºäº†å®Œæ•´çš„æç¤º
        full_prompt = prompt
        
        # è®°å½•å‘é€ç»™APIçš„æç¤ºå†…å®¹ï¼ˆæˆªå–å‰200å­—ç¬¦ï¼‰
        logger.info(f"å‘é€ç»™APIçš„æç¤ºé¢„è§ˆ: {full_prompt[:200]}...")
        
        try:
            # æ”¯æŒä¸åŒçš„APIæ ¼å¼
            if 'gemini' in model_name.lower() or 'google' in api_base_url.lower():
                return await self._call_gemini_api(full_prompt, api_key, api_base_url)
            elif 'openai' in api_base_url.lower():
                return await self._call_openai_api(full_prompt, api_key, api_base_url, model_name)
            else:
                return await self._call_generic_api(full_prompt, api_key, api_base_url, model_name)
        except Exception as e:
            logger.error(f"APIè°ƒç”¨å¼‚å¸¸: {e}")
            return f"APIè°ƒç”¨å¤±è´¥ï¼š{str(e)}"
    
    async def _call_gemini_api(self, prompt: str, api_key: str, api_base_url: str) -> str:
        """è°ƒç”¨Gemini API - ä½¿ç”¨åŒæ­¥æ–¹å¼ç¡®ä¿ç¨³å®šæ€§"""
        import requests
        import json
        import asyncio
        
        # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡ŒåŒæ­¥è¯·æ±‚ï¼Œé¿å…é˜»å¡äº‹ä»¶å¾ªç¯
        def sync_request():
            # Gemini API URLæ ¼å¼ - ä½¿ç”¨v1beta
            if not api_base_url.endswith('/'):
                api_base_url_fixed = api_base_url + '/'
            else:
                api_base_url_fixed = api_base_url
            
            # ä½¿ç”¨é…ç½®ä¸­çš„æ¨¡å‹åç§°
            model_name = self.model_config.get('model_name', 'gemini-pro')
            url = f"{api_base_url_fixed}v1beta/models/{model_name}:generateContent"
            
            # æ­£ç¡®çš„è¯·æ±‚å¤´æ ¼å¼ - ä½¿ç”¨x-goog-api-key
            headers = {
                'Content-Type': 'application/json',
                'x-goog-api-key': api_key
            }
            
            # æ­£ç¡®çš„è¯·æ±‚ä½“æ ¼å¼
            data = {
                "contents": [
                    {
                        "role": "user",
                        "parts": [
                            {
                                "text": prompt
                            }
                        ]
                    }
                ],
                "generationConfig": {
                    "temperature": self.model_config.get('temperature', 0.7),
                    "maxOutputTokens": self.model_config.get('max_tokens', 4096),
                }
            }
            
            logger.info(f"Gemini API URL: {url}")
            logger.info(f"Gemini API Headers: {headers}")
            
            try:
                response = requests.post(url, headers=headers, json=data, timeout=30)
                logger.info(f"Gemini API Response Status: {response.status_code}")
                
                if response.status_code == 200:
                    result = response.json()
                    candidates = result.get('candidates', [])
                    if candidates and candidates[0].get('content'):
                        parts = candidates[0]['content'].get('parts', [])
                        if parts:
                            answer = parts[0].get('text', 'æœªè·å¾—æœ‰æ•ˆå›å¤')
                            logger.info(f"Gemini API Success: {answer[:100]}...")
                            return answer
                    logger.warning("Gemini APIå“åº”æ ¼å¼ä¸æ­£ç¡®")
                    return 'æœªè·å¾—æœ‰æ•ˆå›å¤'
                else:
                    logger.error(f"Gemini APIè¯·æ±‚å¤±è´¥ ({response.status_code}): {response.text}")
                    raise Exception(f"APIè¯·æ±‚å¤±è´¥ ({response.status_code}): {response.text}")
                    
            except Exception as e:
                logger.error(f"Gemini APIè°ƒç”¨å¼‚å¸¸: {e}")
                raise e
        
        # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡ŒåŒæ­¥è¯·æ±‚
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(None, sync_request)
        return result
    
    async def _call_openai_api(self, prompt: str, api_key: str, api_base_url: str, model_name: str) -> str:
        """è°ƒç”¨OpenAI API"""
        import aiohttp
        import json
        
        url = f"{api_base_url}/chat/completions"
        
        headers = {
            'Content-Type': 'application/json',
            'Authorization': f'Bearer {api_key}'
        }
        
        data = {
            "model": model_name,
            "messages": [
                {"role": "user", "content": prompt}
            ],
            "temperature": self.model_config.get('temperature', 0.7),
            "max_tokens": self.model_config.get('max_tokens', 4096),
        }
        
        timeout = aiohttp.ClientTimeout(total=30)
        async with aiohttp.ClientSession(timeout=timeout) as session:
            async with session.post(url, headers=headers, json=data) as response:
                if response.status == 200:
                    result = await response.json()
                    choices = result.get('choices', [])
                    if choices:
                        return choices[0]['message']['content']
                    return 'æœªè·å¾—æœ‰æ•ˆå›å¤'
                else:
                    error_text = await response.text()
                    raise Exception(f"APIè¯·æ±‚å¤±è´¥ ({response.status}): {error_text}")
    
    async def _call_generic_api(self, prompt: str, api_key: str, api_base_url: str, model_name: str) -> str:
        """è°ƒç”¨é€šç”¨API"""
        import aiohttp
        import json
        
        # å°è¯•OpenAIæ ¼å¼
        try:
            return await self._call_openai_api(prompt, api_key, api_base_url, model_name)
        except Exception as e:
            logger.error(f"é€šç”¨APIè°ƒç”¨å¤±è´¥: {e}")
            return f"APIè°ƒç”¨å¤±è´¥ï¼š{str(e)}"
    
    async def generate_response(self, prompt: str, context: str = "") -> Dict:
        """ç”Ÿæˆå›ç­”å¹¶è¿”å›è¯¦ç»†ä¿¡æ¯"""
        import time
        start_time = time.time()
        
        logger.info(f"LLM å¼€å§‹ç”Ÿæˆå›ç­”ï¼Œæ¨¡å‹: {self.model_name}")
        logger.info(f"ä¼ å…¥çš„æç¤ºè¯é•¿åº¦: {len(prompt)} å­—ç¬¦")
        logger.info(f"æç¤ºè¯å‰200å­—ç¬¦: {prompt[:200]}...")
        
        try:
            # æ³¨æ„ï¼šè¿™é‡Œä¸ä¼ é€’contextï¼Œå› ä¸ºpromptå·²ç»åŒ…å«äº†å®Œæ•´çš„æç¤ºè¯
            answer = await self.generate(prompt, "")
            response_time = round((time.time() - start_time), 3)  # ä¿æŒä¸ºç§’ï¼Œä¿ç•™3ä½å°æ•°
            
            logger.info(f"LLM å›ç­”ç”ŸæˆæˆåŠŸï¼Œè€—æ—¶: {response_time}ç§’")
            logger.info(f"å›ç­”é¢„è§ˆ: {answer[:200]}...")
            
            return {
                'answer': answer,
                'response_time': response_time,
                'model_used': f"{self.model_name}",
                'success': True
            }
        except Exception as e:
            response_time = round((time.time() - start_time), 3)  # ä¿æŒä¸ºç§’ï¼Œä¿ç•™3ä½å°æ•°
            logger.error(f"LLM å›ç­”ç”Ÿæˆå¤±è´¥: {str(e)}")
            return {
                'answer': f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {str(e)}",
                'response_time': response_time,
                'model_used': f"{self.model_name}",
                'success': False,
                'error': str(e)
            }


class RAGSystem:
    """RAGç³»ç»Ÿä¸»ç±»"""
    
    def __init__(self):
        self.document_processor = DocumentProcessor()
        self.text_splitter = TextSplitter()
        self.knowledge_bases = {}  # å­˜å‚¨æ¯ä¸ªçŸ¥è¯†åº“çš„å‘é‡å­˜å‚¨
        self.llm_configs = {}  # å­˜å‚¨LLMé…ç½®
        
    def get_or_create_vector_store(self, kb_id: int) -> VectorStore:
        """è·å–æˆ–åˆ›å»ºçŸ¥è¯†åº“çš„å‘é‡å­˜å‚¨"""
        if kb_id not in self.knowledge_bases:
            self.knowledge_bases[kb_id] = VectorStore()
            # æ³¨æ„ï¼šä¸åœ¨è¿™é‡Œè‡ªåŠ¨åŠ è½½æ–‡æ¡£ï¼Œç”±ask_questionæ–¹æ³•æ§åˆ¶åŠ è½½æ—¶æœº
        return self.knowledge_bases[kb_id]
    
    def _load_existing_documents(self, kb_id: int):
        """ä»æ•°æ®åº“åŠ è½½å·²æœ‰çš„æ–‡æ¡£æ•°æ®åˆ°å‘é‡å­˜å‚¨"""
        try:
            from apps.knowledge.models import DocumentChunk
            import asyncio
            
            # æ£€æŸ¥æ˜¯å¦åœ¨å¼‚æ­¥ç¯å¢ƒä¸­
            try:
                loop = asyncio.get_event_loop()
                if loop.is_running():
                    # åœ¨å¼‚æ­¥ç¯å¢ƒä¸­ï¼Œä½¿ç”¨åŒæ­¥æ–¹å¼å¼ºåˆ¶åŠ è½½
                    logger.info(f"åœ¨å¼‚æ­¥ç¯å¢ƒä¸­å¼ºåˆ¶åŠ è½½çŸ¥è¯†åº“ {kb_id} çš„æ•°æ®")
                    # åˆ›å»ºæ–°çš„çº¿ç¨‹æ¥æ‰§è¡ŒåŒæ­¥æ•°æ®åº“æ“ä½œ
                    import threading
                    import queue
                    
                    result_queue = queue.Queue()
                    
                    def sync_load():
                        try:
                            # è·å–çŸ¥è¯†åº“ä¸­æ‰€æœ‰å·²å®Œæˆçš„æ–‡æ¡£å—
                            chunks = DocumentChunk.objects.filter(
                                document__knowledge_base_id=kb_id,
                                document__status='completed'
                            ).select_related('document')
                            
                            chunk_data = []
                            for chunk in chunks:
                                chunk_data.append({
                                    'content': chunk.content,
                                    'metadata': {
                                        'document_id': chunk.document.id,
                                        'chunk_index': chunk.chunk_index,
                                        'source': chunk.document.file_path,
                                        'type': chunk.document.file_type,
                                        **chunk.metadata
                                    }
                                })
                            result_queue.put(chunk_data)
                        except Exception as e:
                            result_queue.put(e)
                    
                    thread = threading.Thread(target=sync_load)
                    thread.start()
                    thread.join()
                    
                    chunk_data = result_queue.get()
                    if isinstance(chunk_data, Exception):
                        raise chunk_data
                    
                    vector_store = self.knowledge_bases[kb_id]
                    for chunk_info in chunk_data:
                        vector_store.chunks.append(chunk_info['content'])
                        vector_store.metadata.append(chunk_info['metadata'])
                    
                    # é‡å»ºå‘é‡
                    if vector_store.chunks:
                        vector_store._update_vectors()
                    
                    logger.info(f"ä»æ•°æ®åº“åŠ è½½çŸ¥è¯†åº“ {kb_id} çš„æ–‡æ¡£æ•°æ®: {len(vector_store.chunks)} ä¸ªå—")
                    return
                    
            except RuntimeError:
                # æ²¡æœ‰äº‹ä»¶å¾ªç¯ï¼Œå¯ä»¥è¿›è¡ŒåŒæ­¥æ“ä½œ
                pass
            
            # åŒæ­¥ç¯å¢ƒä¸­çš„æ­£å¸¸åŠ è½½
            # è·å–çŸ¥è¯†åº“ä¸­æ‰€æœ‰å·²å®Œæˆçš„æ–‡æ¡£å—
            chunks = DocumentChunk.objects.filter(
                document__knowledge_base_id=kb_id,
                document__status='completed'
            ).select_related('document')
            
            vector_store = self.knowledge_bases[kb_id]
            
            for chunk in chunks:
                # æ·»åŠ æ–‡æ¡£å†…å®¹å’Œå…ƒæ•°æ®
                vector_store.chunks.append(chunk.content)
                vector_store.metadata.append({
                    'document_id': chunk.document.id,
                    'chunk_index': chunk.chunk_index,
                    'source': chunk.document.file_path,
                    'type': chunk.document.file_type,
                    **chunk.metadata
                })
            
            # é‡å»ºå‘é‡
            if vector_store.chunks:
                vector_store._update_vectors()
                
            logger.info(f"ä»æ•°æ®åº“åŠ è½½çŸ¥è¯†åº“ {kb_id} çš„æ–‡æ¡£æ•°æ®: {len(vector_store.chunks)} ä¸ªå—")
            
        except Exception as e:
            logger.error(f"åŠ è½½çŸ¥è¯†åº“ {kb_id} çš„æ–‡æ¡£æ•°æ®å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    def manually_load_documents(self, kb_id: int):
        """æ‰‹åŠ¨åŠ è½½çŸ¥è¯†åº“æ–‡æ¡£æ•°æ®ï¼ˆåŒæ­¥æ–¹æ³•ï¼‰- ç¡®ä¿æ¯æ¬¡éƒ½èƒ½æˆåŠŸåŠ è½½"""
        try:
            from apps.knowledge.models import DocumentChunk, Document
            
            logger.info(f"å¼€å§‹å¼ºåˆ¶åŠ è½½çŸ¥è¯†åº“ {kb_id} çš„æ–‡æ¡£æ•°æ®")
            
            # é¦–å…ˆæ£€æŸ¥æ•°æ®åº“ä¸­æ˜¯å¦æœ‰æ–‡æ¡£ - ä½¿ç”¨çº¿ç¨‹å®‰å…¨çš„æ–¹å¼
            doc_count = 0
            try:
                import threading
                count_holder = [0]
                exception_holder = [None]
                
                def count_docs():
                    try:
                        count = Document.objects.filter(
                            knowledge_base_id=kb_id,
                            status='completed'
                        ).count()
                        count_holder[0] = count
                    except Exception as e:
                        exception_holder[0] = e
                
                thread = threading.Thread(target=count_docs)
                thread.start()
                thread.join()
                
                if exception_holder[0]:
                    raise exception_holder[0]
                    
                doc_count = count_holder[0]
                
            except Exception as e:
                logger.error(f"æŸ¥è¯¢æ–‡æ¡£æ•°é‡å¤±è´¥: {e}")
                return 0
            
            logger.info(f"æ•°æ®åº“ä¸­çŸ¥è¯†åº“ {kb_id} æœ‰ {doc_count} ä¸ªå·²å®Œæˆæ–‡æ¡£")
            
            if doc_count == 0:
                logger.info(f"çŸ¥è¯†åº“ {kb_id} ä¸­æ²¡æœ‰å·²å®Œæˆçš„æ–‡æ¡£")
                return 0
            
            # ç¡®ä¿å‘é‡å­˜å‚¨å·²åˆå§‹åŒ–
            if kb_id not in self.knowledge_bases:
                self.knowledge_bases[kb_id] = VectorStore()
                logger.info(f"ä¸ºçŸ¥è¯†åº“ {kb_id} åˆ›å»ºæ–°çš„å‘é‡å­˜å‚¨")
            
            vector_store = self.knowledge_bases[kb_id]
            
            # å¼ºåˆ¶æ¸…ç©ºç°æœ‰æ•°æ®ï¼Œé‡æ–°åŠ è½½ä»¥ç¡®ä¿æ•°æ®åŒæ­¥
            vector_store.chunks.clear()
            vector_store.metadata.clear()
            logger.info(f"æ¸…ç©ºçŸ¥è¯†åº“ {kb_id} çš„ç°æœ‰å‘é‡æ•°æ®")
            
            # è·å–çŸ¥è¯†åº“ä¸­æ‰€æœ‰å·²å®Œæˆçš„æ–‡æ¡£å— - ç®€åŒ–æŸ¥è¯¢å¹¶åŒ…è£…ä¸ºçº¿ç¨‹å®‰å…¨
            try:
                # åœ¨æ–°çº¿ç¨‹ä¸­æ‰§è¡Œæ•°æ®åº“æŸ¥è¯¢ä»¥é¿å…å¼‚æ­¥ä¸Šä¸‹æ–‡é—®é¢˜
                import threading
                chunks_list = []
                exception_holder = [None]
                
                def fetch_chunks():
                    try:
                        chunks = DocumentChunk.objects.filter(
                            document__knowledge_base_id=kb_id,
                            document__status='completed'
                        ).select_related('document').order_by('id')
                        chunks_list.extend(list(chunks))
                    except Exception as e:
                        exception_holder[0] = e
                
                thread = threading.Thread(target=fetch_chunks)
                thread.start()
                thread.join()
                
                if exception_holder[0]:
                    raise exception_holder[0]
                    
                chunks = chunks_list
                
            except Exception as e:
                logger.error(f"æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {e}")
                return 0
            
            chunk_count = 0
            for chunk in chunks:
                try:
                    # æ·»åŠ æ–‡æ¡£å†…å®¹å’Œå…ƒæ•°æ®
                    vector_store.chunks.append(chunk.content)
                    vector_store.metadata.append({
                        'document_id': chunk.document.id,
                        'chunk_index': chunk.chunk_index,
                        'source': chunk.document.file_path,
                        'type': chunk.document.file_type,
                        **chunk.metadata
                    })
                    chunk_count += 1
                    
                    # æ¯å¤„ç†10ä¸ªchunkè®°å½•ä¸€æ¬¡æ—¥å¿—
                    if chunk_count % 10 == 0:
                        logger.info(f"å·²å¤„ç† {chunk_count} ä¸ªæ–‡æ¡£å—")
                        
                except Exception as e:
                    logger.error(f"å¤„ç†æ–‡æ¡£å— {chunk.id} æ—¶å‡ºé”™: {e}")
                    continue
            
            logger.info(f"æ€»å…±å¤„ç†äº† {chunk_count} ä¸ªæ–‡æ¡£å—")
            
            # éªŒè¯æ•°æ®åŠ è½½
            if vector_store.chunks:
                logger.info(f"å‘é‡å­˜å‚¨ä¸­ç°åœ¨æœ‰ {len(vector_store.chunks)} ä¸ªå—")
                logger.info(f"ç¬¬ä¸€ä¸ªå—å†…å®¹é¢„è§ˆ: {vector_store.chunks[0][:100]}...")
                
                # é‡å»ºå‘é‡
                try:
                    vector_store._update_vectors()
                    logger.info(f"çŸ¥è¯†åº“ {kb_id} æˆåŠŸé‡å»ºå‘é‡ç´¢å¼•")
                except Exception as e:
                    logger.error(f"é‡å»ºå‘é‡ç´¢å¼•å¤±è´¥: {e}")
            else:
                logger.error(f"çŸ¥è¯†åº“ {kb_id} åŠ è½½åä»ç„¶æ²¡æœ‰ä»»ä½•æ•°æ®å—ï¼")
            
            logger.info(f"æˆåŠŸå¼ºåˆ¶åŠ è½½çŸ¥è¯†åº“ {kb_id} çš„æ–‡æ¡£æ•°æ®: {chunk_count} ä¸ªå—")
            return chunk_count
            
        except Exception as e:
            logger.error(f"å¼ºåˆ¶åŠ è½½çŸ¥è¯†åº“ {kb_id} çš„æ–‡æ¡£æ•°æ®å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return 0

    def configure_llm(self, config_id: int, model_config: Dict):
        """é…ç½®å¤§è¯­è¨€æ¨¡å‹"""
        self.llm_configs[config_id] = LLMInterface(model_config)
    
    def process_document(self, kb_id: int, file_path: str, document_id: Optional[int] = None) -> Dict:
        """å¤„ç†æ–‡æ¡£"""
        try:
            # å¤„ç†æ–‡æ¡£
            content, metadata = self.document_processor.process_file(file_path)
            
            # ä¸ºæ¯ä¸ªå—æ·»åŠ document_idåˆ°å…ƒæ•°æ®ä¸­
            if document_id:
                metadata['document_id'] = document_id
            
            # åˆ†å—
            chunks = self.text_splitter.split_text(content, metadata)
            
            # è·å–å‘é‡å­˜å‚¨å¹¶æ·»åŠ æ–‡æ¡£
            vector_store = self.get_or_create_vector_store(kb_id)
            vector_store.add_documents(chunks)
            
            return {
                'success': True,
                'chunk_count': len(chunks),
                'content_length': len(content),
                'metadata': metadata
            }
            
        except Exception as e:
            logger.error(f"å¤„ç†æ–‡æ¡£å¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e),
                'chunk_count': 0
            }
    
    async def ask_question(self, kb_id: int, question: str, config_id: Optional[int] = None, 
                          top_k: int = 5, threshold: float = 0.5) -> Dict:
        """æ™ºèƒ½é—®ç­”"""
        import time
        start_time = time.time()
        
        try:
            # è·å–å‘é‡å­˜å‚¨
            vector_store = self.get_or_create_vector_store(kb_id)
            
            # æ¯æ¬¡é—®ç­”éƒ½é‡æ–°æ£€æŸ¥å¹¶åŠ è½½æ–‡æ¡£ï¼Œç¡®ä¿æ–‡æ¡£æ•°æ®æ˜¯æœ€æ–°çš„
            logger.info(f"çŸ¥è¯†åº“ {kb_id} å¼€å§‹æ£€æŸ¥æ–‡æ¡£çŠ¶æ€")
            
            # å¼ºåˆ¶é‡æ–°åŠ è½½æ–‡æ¡£æ•°æ®ä»¥ç¡®ä¿æ•°æ®å®Œæ•´æ€§
            loaded_count = self.manually_load_documents(kb_id)
            logger.info(f"çŸ¥è¯†åº“ {kb_id} é‡æ–°åŠ è½½äº† {loaded_count} ä¸ªæ–‡æ¡£å—")
            
            # é‡æ–°è·å–å‘é‡å­˜å‚¨ï¼ˆç¡®ä¿è·å–æœ€æ–°æ•°æ®ï¼‰
            vector_store = self.get_or_create_vector_store(kb_id)
            
            # æ£€ç´¢ç›¸å…³æ–‡æ¡£ - ä½¿ç”¨æ›´ä½çš„é˜ˆå€¼ç¡®ä¿èƒ½æ£€ç´¢åˆ°æ–‡æ¡£
            relevant_docs = vector_store.similarity_search(question, top_k=top_k, threshold=max(threshold, 0.1))
            
            logger.info(f"æ£€ç´¢åˆ° {len(relevant_docs)} ä¸ªç›¸å…³æ–‡æ¡£ç‰‡æ®µï¼Œé˜ˆå€¼: {max(threshold, 0.1)}")
            
            # å¦‚æœæ²¡æœ‰æ£€ç´¢åˆ°æ–‡æ¡£ï¼Œå°è¯•é™ä½é˜ˆå€¼å†æ¬¡æ£€ç´¢
            if not relevant_docs and threshold > 0.0:
                logger.info("æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼Œå°è¯•é™ä½é˜ˆå€¼é‡æ–°æ£€ç´¢")
                relevant_docs = vector_store.similarity_search(question, top_k=top_k, threshold=0.0)
                logger.info(f"é™ä½é˜ˆå€¼åæ£€ç´¢åˆ° {len(relevant_docs)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
            
            # æ„å»ºä¸Šä¸‹æ–‡ - å¼ºåˆ¶ä½¿ç”¨çŸ¥è¯†åº“å†…å®¹ï¼Œç¡®ä¿æ€»æ˜¯æœ‰å†…å®¹
            context = ""
            context_info = ""
            
            # é¦–å…ˆå°è¯•ä½¿ç”¨æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£
            if relevant_docs:
                context = "\n".join([doc['content'] for doc in relevant_docs])
                context_info = f"åŸºäºçŸ¥è¯†åº“ä¸­çš„ {len(relevant_docs)} ä¸ªç›¸å…³æ–‡æ¡£ç‰‡æ®µï¼š"
                logger.info(f"ä½¿ç”¨ç›¸å…³æ–‡æ¡£æ„å»ºä¸Šä¸‹æ–‡ï¼Œé•¿åº¦: {len(context)} å­—ç¬¦")
            
            # å¦‚æœæ²¡æœ‰ç›¸å…³æ–‡æ¡£ä½†æœ‰çŸ¥è¯†åº“å†…å®¹ï¼Œå¼ºåˆ¶ä½¿ç”¨å‰å‡ ä¸ªå—
            if not context and vector_store.chunks:
                logger.info("æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼Œå¼ºåˆ¶ä½¿ç”¨çŸ¥è¯†åº“å‰å‡ ä¸ªæ–‡æ¡£å—")
                context = "\n".join(vector_store.chunks[:min(10, len(vector_store.chunks))])
                context_info = f"åŸºäºçŸ¥è¯†åº“ä¸­çš„å‰ {min(10, len(vector_store.chunks))} ä¸ªæ–‡æ¡£ç‰‡æ®µï¼š"
                logger.info(f"å¼ºåˆ¶æ„å»ºçš„ä¸Šä¸‹æ–‡é•¿åº¦: {len(context)} å­—ç¬¦")
                
                # åŒæ—¶å°†å‰å‡ ä¸ªå—å½“ä½œrelevant_docså¤„ç†ï¼Œä¿è¯åç»­é€»è¾‘æ­£ç¡®
                relevant_docs = []
                for i, chunk in enumerate(vector_store.chunks[:min(10, len(vector_store.chunks))]):
                    relevant_docs.append({
                        'content': chunk,
                        'score': 0.1,  # ç»™ä¸€ä¸ªé»˜è®¤åˆ†æ•°
                        'metadata': vector_store.metadata[i] if i < len(vector_store.metadata) else {},
                        'index': i
                    })
            
            # æœ€åçš„ä¿é™©ï¼šå¦‚æœä»ç„¶æ²¡æœ‰contextï¼Œæ£€æŸ¥æ˜¯å¦çœŸçš„æ²¡æœ‰æ•°æ®
            if not context:
                # å†æ¬¡å°è¯•ç›´æ¥ä»æ•°æ®åº“è·å–ä¸€äº›å†…å®¹
                try:
                    from apps.knowledge.models import DocumentChunk
                    import threading
                    
                    db_content = []
                    db_chunks_data = []
                    exception_holder = [None]
                    
                    def fetch_db_chunks():
                        try:
                            db_chunks = DocumentChunk.objects.filter(
                                document__knowledge_base_id=kb_id,
                                document__status='completed'
                            ).select_related('document')[:5]
                            
                            for chunk in db_chunks:
                                db_content.append(chunk.content)
                                db_chunks_data.append(chunk)
                        except Exception as e:
                            exception_holder[0] = e
                    
                    thread = threading.Thread(target=fetch_db_chunks)
                    thread.start()
                    thread.join()
                    
                    if exception_holder[0]:
                        raise exception_holder[0]
                    
                    if db_content:
                        logger.warning("å‘é‡å­˜å‚¨ä¸ºç©ºä½†æ•°æ®åº“æœ‰æ•°æ®ï¼Œç›´æ¥ä»æ•°æ®åº“è·å–")
                        context = "\n".join(db_content)
                        context_info = f"ç›´æ¥ä»æ•°æ®åº“è·å–çš„ {len(db_content)} ä¸ªæ–‡æ¡£ç‰‡æ®µï¼š"
                        logger.info(f"ä»æ•°æ®åº“ç›´æ¥è·å–çš„ä¸Šä¸‹æ–‡é•¿åº¦: {len(context)} å­—ç¬¦")
                        
                        # æ„é€ ç›¸åº”çš„relevant_docs
                        relevant_docs = []
                        for i, chunk in enumerate(db_chunks_data):
                            relevant_docs.append({
                                'content': chunk.content,
                                'score': 0.05,  # æ›´ä½çš„åˆ†æ•°è¡¨ç¤ºè¿™æ˜¯ç›´æ¥è·å–çš„
                                'metadata': {'document_id': chunk.document.id, 'chunk_index': chunk.chunk_index},
                                'index': i
                            })
                    else:
                        context = ""
                        context_info = "çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–‡æ¡£"
                        logger.error(f"çŸ¥è¯†åº“ {kb_id} æ•°æ®åº“ä¸­ä¹Ÿæ²¡æœ‰ä»»ä½•æ–‡æ¡£å†…å®¹")
                except Exception as e:
                    logger.error(f"ä»æ•°æ®åº“è·å–å¤‡ç”¨å†…å®¹å¤±è´¥: {e}")
                    context = ""
                    context_info = "çŸ¥è¯†åº“è¯»å–å¤±è´¥"
            
            # è®°å½•æœ€ç»ˆçš„contextçŠ¶æ€
            logger.info(f"æœ€ç»ˆcontextçŠ¶æ€: é•¿åº¦={len(context)}, ä¿¡æ¯={context_info}")
            if context:
                logger.info(f"Contextå‰200å­—ç¬¦: {context[:200]}...")
            
            # ç”Ÿæˆå›ç­” - ç¡®ä¿æ€»æ˜¯å°†çŸ¥è¯†åº“å†…å®¹ä¼ é€’ç»™å¤§æ¨¡å‹
            llm = self.llm_configs.get(config_id) if config_id else None
            if llm:
                logger.info(f"ä½¿ç”¨LLMé…ç½®ID: {config_id}")
                logger.info(f"æ£€æŸ¥ä¸Šä¸‹æ–‡çŠ¶æ€: contexté•¿åº¦={len(context) if context else 0}, vector_store.chunksæ•°é‡={len(vector_store.chunks)}, relevant_docsæ•°é‡={len(relevant_docs)}")
                
                # æœ€åçš„å¼ºåˆ¶ä¿é™©ï¼šå¦‚æœcontextä»ç„¶ä¸ºç©ºï¼Œç›´æ¥ä»æ•°æ®åº“å¼ºåˆ¶è·å–
                if not context:
                    logger.error("ä¸¥é‡è­¦å‘Š: contextä¸ºç©ºï¼Œæ‰§è¡Œæœ€ç»ˆå…œåº•æ“ä½œ")
                    try:
                        from apps.knowledge.models import DocumentChunk
                        import threading
                        
                        emergency_content = []
                        exception_holder = [None]
                        
                        def fetch_emergency_chunks():
                            try:
                                emergency_chunks = DocumentChunk.objects.filter(
                                    document__knowledge_base_id=kb_id,
                                    document__status='completed'
                                )[:3]
                                
                                for chunk in emergency_chunks:
                                    emergency_content.append(chunk.content)
                            except Exception as e:
                                exception_holder[0] = e
                        
                        thread = threading.Thread(target=fetch_emergency_chunks)
                        thread.start()
                        thread.join()
                        
                        if exception_holder[0]:
                            raise exception_holder[0]
                        
                        if emergency_content:
                            context = "\n".join(emergency_content)
                            logger.error(f"ç´§æ€¥å…œåº•: ä»æ•°æ®åº“è·å–åˆ° {len(emergency_content)} ä¸ªå—")
                    except Exception as emergency_e:
                        logger.error(f"ç´§æ€¥å…œåº•ä¹Ÿå¤±è´¥: {emergency_e}")
                
                # ç°åœ¨contextåº”è¯¥æ€»æ˜¯æœ‰å†…å®¹ï¼ˆé™¤éçŸ¥è¯†åº“çœŸçš„ä¸ºç©ºï¼‰
                if context:
                    logger.info(f"ä½¿ç”¨æœ‰å†…å®¹çš„contextæ„å»ºæç¤ºè¯ï¼Œcontextå‰100å­—ç¬¦: {context[:100]}")
                    # æ„å»ºæå…¶æ˜ç¡®çš„æç¤ºï¼Œå¼ºåˆ¶å¤§æ¨¡å‹æŒ‰æ ¼å¼å›ç­”
                    enhanced_question = f"""ã€ä¸¥æ ¼æŒ‡ä»¤ - å¿…é¡»éµå®ˆã€‘ä½ æ˜¯ä¸“ä¸šçŸ¥è¯†åº“åŠ©æ‰‹ï¼Œå¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼å›ç­”ï¼Œä¸å¾—è¿åï¼š

ğŸ”´ å¼ºåˆ¶è¦æ±‚ï¼š
1. ç¬¬ä¸€å¥è¯å¿…é¡»æ˜¯ï¼š"åŸºäºçŸ¥è¯†åº“å†…å®¹ï¼Œæˆ‘ä¸ºæ‚¨å›ç­”ï¼š"
2. ç¦æ­¢ä½¿ç”¨"åŸºäºé€šç”¨çŸ¥è¯†"ç­‰å…¶ä»–å¼€å¤´
3. å¿…é¡»å¼•ç”¨ä¸‹é¢çš„çŸ¥è¯†åº“å†…å®¹
4. ä¸å¾—è¯´"æ²¡æœ‰ç›¸å…³å†…å®¹"

ğŸ“š çŸ¥è¯†åº“å†…å®¹ï¼š
{context}

â“ ç”¨æˆ·é—®é¢˜ï¼š{question}

âœ… å›ç­”æ ¼å¼ç¤ºä¾‹ï¼š
åŸºäºçŸ¥è¯†åº“å†…å®¹ï¼Œæˆ‘ä¸ºæ‚¨å›ç­”ï¼š[æ ¹æ®ä¸Šè¿°çŸ¥è¯†åº“å†…å®¹çš„å…·ä½“å›ç­”]

âš ï¸ é‡è¦æé†’ï¼šæ— è®ºå¦‚ä½•éƒ½å¿…é¡»ä»¥"åŸºäºçŸ¥è¯†åº“å†…å®¹ï¼Œæˆ‘ä¸ºæ‚¨å›ç­”ï¼š"å¼€å¤´ï¼Œè¿™æ˜¯ä¸å¯è¿åçš„è§„åˆ™ï¼

ç°åœ¨è¯·ä¸¥æ ¼æŒ‰ç…§æ ¼å¼å¼€å§‹å›ç­”ï¼š"""
                else:
                    # è¿™ç§æƒ…å†µç°åœ¨åº”è¯¥æå°‘å‘ç”Ÿ
                    logger.error("å³ä½¿ç»è¿‡æ‰€æœ‰å…œåº•æªæ–½ï¼Œcontextä»ç„¶ä¸ºç©ºï¼è¿™ä¸åº”è¯¥å‘ç”Ÿã€‚")
                    enhanced_question = f"""ã€ä¸¥æ ¼æŒ‡ä»¤ã€‘çŸ¥è¯†åº“åŠ©æ‰‹å¿…é¡»å›ç­”ï¼š

ç¬¬ä¸€å¥è¯å¿…é¡»æ˜¯ï¼š"åŸºäºçŸ¥è¯†åº“å†…å®¹ï¼Œæˆ‘ä¸ºæ‚¨å›ç­”ï¼š"
ç„¶åè¯´æ˜ï¼š"å½“å‰çŸ¥è¯†åº“ç³»ç»Ÿå‡ºç°é—®é¢˜ï¼Œæ— æ³•è¯»å–æ–‡æ¡£å†…å®¹ã€‚"

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°æ ¼å¼å›ç­”ã€‚"""
                
                logger.info(f"å‘é€ç»™å¤§æ¨¡å‹çš„å®Œæ•´æç¤ºé•¿åº¦: {len(enhanced_question)} å­—ç¬¦")
                logger.info(f"ä¸Šä¸‹æ–‡å†…å®¹é¢„è§ˆ: {context[:300]}..." if context else "ä¸Šä¸‹æ–‡ä¸ºç©º")
                
                # ä½¿ç”¨æ„å»ºå¥½çš„å®Œæ•´æç¤ºè¯
                llm_result = await llm.generate_response(enhanced_question, "")
                answer = llm_result.get('answer', 'ç”Ÿæˆå›ç­”å¤±è´¥')
                model_used = llm_result.get('model_used', f"config_{config_id}" if config_id else "default")
            else:
                logger.warning(f"æ²¡æœ‰æ‰¾åˆ°LLMé…ç½®ï¼Œé…ç½®ID: {config_id}")
                # å¦‚æœæ²¡æœ‰é…ç½®LLMï¼Œæ‰è¿”å›"æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯"çš„æç¤º
                if not relevant_docs:
                    response_time = round((time.time() - start_time), 3)
                    return {
                        'answer': 'æŠ±æ­‰ï¼Œæˆ‘åœ¨çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ï¼Œä¸”æœªé…ç½®å¤§è¯­è¨€æ¨¡å‹ã€‚è¯·é…ç½®æ¨¡å‹ä»¥è·å¾—æ™ºèƒ½å›ç­”ã€‚',
                        'sources': [],
                        'confidence': 0.0,
                        'retrieved_chunks': [],
                        'model_used': f"config_{config_id}" if config_id else "default",
                        'response_time': response_time
                    }
                else:
                    answer = f"åŸºäºçŸ¥è¯†åº“å†…å®¹ï¼Œæ‰¾åˆ°äº† {len(relevant_docs)} ä¸ªç›¸å…³ç‰‡æ®µï¼Œä½†æœªé…ç½®å¤§è¯­è¨€æ¨¡å‹ã€‚è¯·é…ç½®æ¨¡å‹ä»¥è·å¾—æ™ºèƒ½å›ç­”ã€‚"
                    model_used = f"config_{config_id}" if config_id else "default"
            
            response_time = round((time.time() - start_time), 3)  # ä¿æŒä¸ºç§’ï¼Œä¿ç•™3ä½å°æ•°
            
            logger.info(f"é—®ç­”å®Œæˆ: å›ç­”é•¿åº¦={len(answer)}, æºæ–‡æ¡£æ•°={len(relevant_docs)}, ä½¿ç”¨æ¨¡å‹={model_used}, å“åº”æ—¶é—´={response_time}ç§’")
            
            return {
                'answer': answer,
                'sources': [
                    {
                        'content': doc['content'][:200] + '...' if len(doc['content']) > 200 else doc['content'],
                        'score': doc['score'],
                        'metadata': doc['metadata']
                    }
                    for doc in relevant_docs
                ],
                'confidence': relevant_docs[0]['score'] if relevant_docs else 0.0,
                'retrieved_chunks': relevant_docs,
                'model_used': model_used,
                'response_time': response_time
            }
            
        except Exception as e:
            logger.error(f"é—®ç­”å¤±è´¥: {e}")
            response_time = round((time.time() - start_time), 3)  # ä¿æŒä¸ºç§’ï¼Œä¿ç•™3ä½å°æ•°
            return {
                'answer': f'æŸ¥è¯¢è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}',
                'sources': [],
                'confidence': 0.0,
                'retrieved_chunks': [],
                'model_used': "error",
                'response_time': response_time
            }
    
    def get_knowledge_base_stats(self, kb_id: int) -> Dict:
        """è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯"""
        if kb_id in self.knowledge_bases:
            vector_store = self.knowledge_bases[kb_id]
            return {
                'total_chunks': len(vector_store.chunks),
                'total_documents': len(set(chunk.get('document_id', 0) for chunk in vector_store.metadata)),
                'vector_dimension': vector_store.vectors.shape[1] if vector_store.vectors is not None else 0
            }
        else:
            return {
                'total_chunks': 0,
                'total_documents': 0,
                'vector_dimension': 0
            }
